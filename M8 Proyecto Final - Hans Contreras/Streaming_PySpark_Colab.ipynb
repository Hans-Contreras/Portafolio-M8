{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f1ec193c",
      "metadata": {
        "id": "f1ec193c"
      },
      "source": [
        "# üöá M√≥dulo 8 ‚Äì Streaming con PySpark (Google Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae024958",
      "metadata": {
        "id": "ae024958"
      },
      "source": [
        "## 1) Instalar y configurar Spark en Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0520f63d"
      },
      "outputs": [],
      "source": [
        "!apt-get install -qq openjdk-11-jdk > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.5.7/spark-3.5.7-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.7-bin-hadoop3.tgz\n",
        "!pip -q install pyspark findspark"
      ],
      "id": "0520f63d"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b21ea264",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b21ea264",
        "outputId": "4cae5d81-4538-4966-a546-f2c2ac79ccba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAVA_HOME = /usr/lib/jvm/java-11-openjdk-amd64\n",
            "SPARK_HOME = /content/spark-3.5.7-bin-hadoop3\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['JAVA_HOME']='/usr/lib/jvm/java-11-openjdk-amd64'\n",
        "os.environ['SPARK_HOME']='/content/spark-3.5.7-bin-hadoop3'\n",
        "print('JAVA_HOME =', os.environ['JAVA_HOME'])\n",
        "print('SPARK_HOME =', os.environ['SPARK_HOME'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b873c918",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b873c918",
        "outputId": "5cd7e270-d2c5-4096-cf51-b2b91eb246e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Spark version: 3.5.7\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('Mod8-Streaming').getOrCreate()\n",
        "print('‚úÖ Spark version:', spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32f169a4",
      "metadata": {
        "id": "32f169a4"
      },
      "source": [
        "## 2) Preparar carpeta de entrada y datos de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "03b6fa5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03b6fa5a",
        "outputId": "4ba5aa65-60da-471c-e7e3-780bb5d5612a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ CSV de prueba creado\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p /content/stream_inputs\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "now = datetime.now()\n",
        "df = pd.DataFrame({'timestamp':[now, now+timedelta(minutes=1), now+timedelta(minutes=2)],'linea':['L1','L1','L2'],'estacion':['Los H√©roes','Baquedano','Tobalaba'],'afluencia':[120,300,180]})\n",
        "df.to_csv('/content/stream_inputs/batch1.csv', index=False)\n",
        "print('‚úÖ CSV de prueba creado')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33ea30cd",
      "metadata": {
        "id": "33ea30cd"
      },
      "source": [
        "## 3) Lectura con Structured Streaming + agregaci√≥n por ventanas (5 min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5ba508f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ba508f4",
        "outputId": "c50188c6-db61-4147-fb29-4ef13511f530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ñ∂Ô∏è Streaming iniciado\n",
            "‚èπÔ∏è Streaming detenido\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
        "from pyspark.sql.functions import col, window, sum as _sum\n",
        "schema = StructType([StructField('timestamp', TimestampType(), True),StructField('linea', StringType(), True),StructField('estacion', StringType(), True),StructField('afluencia', IntegerType(), True)])\n",
        "df_stream = (spark.readStream.schema(schema).option('maxFilesPerTrigger', 1).csv('/content/stream_inputs'))\n",
        "agg = (df_stream.withWatermark('timestamp','10 minutes').groupBy(window(col('timestamp'),'5 minutes'), col('estacion')).agg(_sum('afluencia').alias('pasajeros')))\n",
        "query = (agg.writeStream.outputMode('update').format('console').option('truncate', False).start())\n",
        "print('‚ñ∂Ô∏è Streaming iniciado'); query.awaitTermination(20); query.stop(); print('‚èπÔ∏è Streaming detenido')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d3df37f",
      "metadata": {
        "id": "0d3df37f"
      },
      "source": [
        "## 4) Clasificaci√≥n binaria (hora punta) ‚Äì entrenamiento offline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c1c6ada7",
      "metadata": {
        "id": "c1c6ada7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3ea77d-52f7-449e-9cce-ba2a64e0fdf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+\n",
            "|  estacion|pasajeros|hora_punta|\n",
            "+----------+---------+----------+\n",
            "|Los H√©roes|      100|         0|\n",
            "|Los H√©roes|      350|         1|\n",
            "|Los H√©roes|      220|         1|\n",
            "| Baquedano|       90|         0|\n",
            "| Baquedano|      310|         1|\n",
            "| Baquedano|      180|         0|\n",
            "|  Tobalaba|       80|         0|\n",
            "|  Tobalaba|      190|         0|\n",
            "|  Tobalaba|      400|         1|\n",
            "+----------+---------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "hist = spark.createDataFrame([('Los H√©roes',100,0),('Los H√©roes',350,1),('Los H√©roes',220,1),('Baquedano',90,0),('Baquedano',310,1),('Baquedano',180,0),('Tobalaba',80,0),('Tobalaba',190,0),('Tobalaba',400,1)], ['estacion','pasajeros','hora_punta'])\n",
        "hist.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "174a4e04",
      "metadata": {
        "id": "174a4e04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "468d2680-7bdc-4d36-fa92-b8ba76b866d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 1.0 | Accuracy: 0.5\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "label_col='hora_punta'\n",
        "indexer=StringIndexer(inputCol='estacion', outputCol='estacion_idx', handleInvalid='keep')\n",
        "encoder=OneHotEncoder(inputCols=['estacion_idx'], outputCols=['estacion_ohe'])\n",
        "assembler=VectorAssembler(inputCols=['pasajeros','estacion_ohe'], outputCol='features_raw')\n",
        "scaler=StandardScaler(inputCol='features_raw', outputCol='features')\n",
        "lr=LogisticRegression(featuresCol='features', labelCol=label_col)\n",
        "pipe=Pipeline(stages=[indexer,encoder,assembler,scaler,lr])\n",
        "train,test=hist.randomSplit([0.8,0.2],seed=42)\n",
        "model=pipe.fit(train)\n",
        "pred=model.transform(test)\n",
        "auc=BinaryClassificationEvaluator(labelCol=label_col, metricName='areaUnderROC').evaluate(pred)\n",
        "acc=MulticlassClassificationEvaluator(labelCol=label_col, metricName='accuracy').evaluate(pred)\n",
        "print('AUC:', round(auc,3),'| Accuracy:', round(acc,3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de90cbb6",
      "metadata": {
        "id": "de90cbb6"
      },
      "source": [
        "## 5) Guardar artefactos y modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c3737de4",
      "metadata": {
        "id": "c3737de4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2e3095b-3a51-44e5-fba5-9c21365c6c09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Guardados artefactos y modelo\n"
          ]
        }
      ],
      "source": [
        "import json, os\n",
        "os.makedirs('/content/artifacts', exist_ok=True)\n",
        "os.makedirs('/content/models', exist_ok=True)\n",
        "with open('/content/artifacts/stream_cls_metrics.json','w') as f:\n",
        "    json.dump({'auc': float(auc), 'accuracy': float(acc)}, f, indent=2)\n",
        "model.write().overwrite().save('/content/models/stream_lr_model')\n",
        "print('‚úÖ Guardados artefactos y modelo')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}